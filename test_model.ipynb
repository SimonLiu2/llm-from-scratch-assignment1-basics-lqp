{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from collections.abc import Iterable\n",
    "from typing import IO, Any, BinaryIO\n",
    "\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "from einops import einsum, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.9765, -1.2592,  1.3626, -1.7853, -1.0226],\n",
      "        [-0.2771,  4.2815,  1.6222,  3.2386, -2.3127],\n",
      "        [ 1.6991, -1.0844, -1.6868, -2.4434,  0.2129]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = factory_kwargs\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        nn.init.trunc_normal_(self.weights)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        #using torch.nn.init.trunc_normal_ to initialize weights\n",
    "        output = einsum(input, self.weights, '... d_in, d_out d_in -> ... d_out')\n",
    "        return output\n",
    "    \n",
    "linear = Linear(10, 5)\n",
    "input_tensor = torch.randn(3, 10)\n",
    "output_tensor = linear(input_tensor)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = factory_kwargs\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weights = nn.Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs))\n",
    "        nn.init.trunc_normal_(self.weights)\n",
    "    \n",
    "    def forward(self, token_ids: Tensor) -> Tensor:\n",
    "        # token_ids shape: (batch_size, seq_length)\n",
    "        input = token_ids.long()\n",
    "        output = self.weights[input]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5095,  0.8736,  0.8067,  0.9645, -1.0628,  1.5417, -0.3999, -0.0769,\n",
      "         -0.6319, -1.1427],\n",
      "        [ 1.1821,  0.8556,  0.6303, -0.8412, -1.9316,  0.4741, -0.3189,  0.7791,\n",
      "         -1.4013, -0.3703],\n",
      "        [-0.7361,  0.0817, -1.9319,  1.7296,  1.0280, -0.8135,  0.3896, -0.2692,\n",
      "          0.8007, -0.3789]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = factory_kwargs\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones((dim,), **factory_kwargs))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "        rms_x = norm_x / (x.shape[-1] ** 0.5)\n",
    "        x_normalized = x / (rms_x + self.eps)\n",
    "        output = x_normalized * self.scale\n",
    "        return output.to(in_dtype)\n",
    "rmsnorm = RMSNorm(10)\n",
    "input_tensor = torch.randn(3, 10)\n",
    "output_tensor = rmsnorm(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):\n",
    "    def __init__(self, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = factory_kwargs\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_ff: Int, d_model: Int, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = factory_kwargs\n",
    "        self.linear1 = Linear(d_model, d_ff, \n",
    "                          device=self.factory_kwargs['device'], \n",
    "                          dtype=self.factory_kwargs['dtype'])\n",
    "        self.linear2 = Linear(d_ff, d_model,\n",
    "                            device=self.factory_kwargs['device'], \n",
    "                            dtype=self.factory_kwargs['dtype'])\n",
    "        self.linear3 = Linear(d_model, d_ff,\n",
    "                            device=self.factory_kwargs['device'], \n",
    "                            dtype=self.factory_kwargs['dtype'])\n",
    "        self.activation = SiLU(device=self.factory_kwargs['device'], \n",
    "                              dtype=self.factory_kwargs['dtype'])\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x1 = self.linear1(x)\n",
    "        x2 = self.linear3(x)\n",
    "        activated = self.activation(x1)\n",
    "        gated = activated * x2\n",
    "        output = self.linear2(gated)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        self.factory_kwargs = {'device': device}\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.theta = theta\n",
    "        self.d_k = d_k\n",
    "        self.inv_freq = 1.0 / (theta ** (torch.arange(0, d_k, 2, **self.factory_kwargs) / d_k))\n",
    "        self.register_buffer('cos_cached', torch.zeros((max_seq_len, d_k), **self.factory_kwargs))\n",
    "        self.register_buffer('sin_cached', torch.zeros((max_seq_len, d_k), **self.factory_kwargs))\n",
    "\n",
    "    def forward(self, x: Tensor, token_positions: Tensor) -> Tensor:\n",
    "        seq_len = x.shape[-2]\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds maximum {self.max_seq_len}\")\n",
    "\n",
    "        t = token_positions[:, :seq_len]\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos_emb = emb.cos()[None, :, :]\n",
    "        sin_emb = emb.sin()[None, :, :]\n",
    "        x_rotated = (x * cos_emb) + (self.rotate_half(x) * sin_emb)\n",
    "        return x_rotated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一行第一列保持为True\n",
    "x = torch.randn(2, 8, 8)\n",
    "seq_len = x.size(1)\n",
    "mask = torch.tril(torch.ones((seq_len, seq_len), device=x.device), diagonal=0).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.035511016845703\n",
      "23.7292537689209\n",
      "20.084440231323242\n",
      "16.999469757080078\n",
      "14.388352394104004\n",
      "12.178300857543945\n",
      "10.307714462280273\n",
      "8.724449157714844\n",
      "7.384374618530273\n",
      "6.250134468078613\n",
      "5.29011344909668\n",
      "4.47755241394043\n",
      "3.7898004055023193\n",
      "3.20768666267395\n",
      "2.714986562728882\n",
      "2.297964334487915\n",
      "1.9449970722198486\n",
      "1.6462457180023193\n",
      "1.3933823108673096\n",
      "1.1793588399887085\n",
      "0.9982093572616577\n",
      "0.8448843955993652\n",
      "0.7151100039482117\n",
      "0.605269193649292\n",
      "0.5122998356819153\n",
      "0.43361055850982666\n",
      "0.3670079708099365\n",
      "0.31063559651374817\n",
      "0.26292192935943604\n",
      "0.22253715991973877\n",
      "0.1883554458618164\n",
      "0.1594240367412567\n",
      "0.13493651151657104\n",
      "0.11421027034521103\n",
      "0.09666755795478821\n",
      "0.08181943744421005\n",
      "0.06925196945667267\n",
      "0.05861486867070198\n",
      "0.04961162060499191\n",
      "0.041991278529167175\n",
      "0.03554141893982887\n",
      "0.030082259327173233\n",
      "0.025461621582508087\n",
      "0.02155071683228016\n",
      "0.01824052631855011\n",
      "0.01543878111988306\n",
      "0.013067386113107204\n",
      "0.01106023509055376\n",
      "0.009361382573843002\n",
      "0.007923474535346031\n",
      "0.0067064277827739716\n",
      "0.0056763202883303165\n",
      "0.004804438445717096\n",
      "0.004066476598381996\n",
      "0.003441866021603346\n",
      "0.0029131954070180655\n",
      "0.002465728437528014\n",
      "0.002086992608383298\n",
      "0.0017664306797087193\n",
      "0.0014951068442314863\n",
      "0.001265458413399756\n",
      "0.0010710840579122305\n",
      "0.0009065655758604407\n",
      "0.0007673171348869801\n",
      "0.0006494572153314948\n",
      "0.000549700518604368\n",
      "0.0004652665229514241\n",
      "0.00039380163070745766\n",
      "0.0003333137137815356\n",
      "0.0002821167290676385\n",
      "0.00023878358479123563\n",
      "0.00020210642833262682\n",
      "0.0001710628712316975\n",
      "0.00014478762750513852\n",
      "0.00012254824105184525\n",
      "0.00010372482938691974\n",
      "8.779269410297275e-05\n",
      "7.430773985106498e-05\n",
      "6.289406883297488e-05\n",
      "5.32335507159587e-05\n",
      "4.505687320488505e-05\n",
      "3.8136135117383674e-05\n",
      "3.227842535125092e-05\n",
      "2.732045868469868e-05\n",
      "2.312403557880316e-05\n",
      "1.9572185919969343e-05\n",
      "1.6565896657994017e-05\n",
      "1.4021376046002842e-05\n",
      "1.186769350169925e-05\n",
      "1.0044814189313911e-05\n",
      "8.501931006321684e-06\n",
      "7.196034857770428e-06\n",
      "6.090723218221683e-06\n",
      "5.155188773642294e-06\n",
      "4.3633517634589225e-06\n",
      "3.6931410249962937e-06\n",
      "3.1258744002116146e-06\n",
      "2.6457400963408872e-06\n",
      "2.2393544441001723e-06\n",
      "1.8953899143525632e-06\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5*torch.randn(5, 5))\n",
    "opt = torch.optim.SGD([weights], lr=1)\n",
    "for _ in range(100):\n",
    "    opt.zero_grad()\n",
    "    loss = (weights**2).mean()\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4734, -0.8047,  0.9079,  1.0118],\n",
       "        [ 1.7014, -0.2291,  0.8223, -0.1887],\n",
       "        [-0.6490, -0.8112, -1.0593,  0.4451],\n",
       "        [ 0.9344,  0.0696,  1.5256,  1.1624],\n",
       "        [ 0.5979,  0.2832, -0.1796, -0.8682],\n",
       "        [-0.7022, -0.2886,  0.1699,  1.2305]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "outputs = torch.randn(2, 3, 4)# (batch_size, seq_length, feature_dim)\n",
    "targets = torch.randn(2, 3) # (batch_size, seq_length)\n",
    "print(outputs.view(-1, outputs.size(-1)))  # Reshape to (batch_size * seq_length, feature_dim)\n",
    "print(targets.view(-1))  # Reshape to (batch_size * seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/home/std10/extend/generated_data/tokenized_data_train.npy', allow_pickle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
