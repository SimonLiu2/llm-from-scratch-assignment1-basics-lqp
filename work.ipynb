{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import regex as re\n",
    "from collections.abc import Iterable\n",
    "from typing import IO, Any, BinaryIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # 1. Pre-tokenization\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    pre_tokens_dict: dict[tuple[bytes], int] = {}\n",
    "    # Parallel processing of chunks\n",
    "        \n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_process = 10\n",
    "        boundaries = find_chunk_boundaries(f, num_process, b'<|endoftext|>')\n",
    "        # Single threaded processing for comparison\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode('utf-8', errors='ignore')\n",
    "            #split the chunk on special tokens\n",
    "            parts = re.split(\"|\".join(special_tokens), chunk)\n",
    "            for part in parts:\n",
    "                tokens = re.finditer(PAT, part)\n",
    "                for match in tokens:\n",
    "                    token_bytes = match.group(0).encode('utf-8')\n",
    "                    token_tuple = (token_bytes)\n",
    "                    pre_tokens_dict[token_tuple] = pre_tokens_dict.get(token_tuple, 0) + 1\n",
    "    vocab = {}\n",
    "    merges = []\n",
    "    #bulid initial vocab with special tokens and ascii characters\n",
    "    for i in range(256):\n",
    "        vocab[i] = bytes([i])\n",
    "    for i, special_token in enumerate(special_tokens):\n",
    "        vocab[i+256] = special_token.encode('utf-8')\n",
    "    \n",
    "    current_size = 256 + len(special_tokens)\n",
    "    current_tokens = {}  #store current tokens to be merged\n",
    "    time_taken1 = 0.0\n",
    "    time_taken2 = 0.0\n",
    "    pair_counts: dict[tuple[bytes, bytes], int] = {}\n",
    "    loop_number = 0\n",
    "    while(current_size < vocab_size):\n",
    "\n",
    "        start_time1 = time.time()\n",
    "        if loop_number == 0:\n",
    "            for key in pre_tokens_dict.keys():\n",
    "                if key not in current_tokens:\n",
    "                    current_tokens[key] = list(key)\n",
    "                tokens = current_tokens[key]\n",
    "                for i in range(len(tokens)-1):\n",
    "                    pair  = (tokens[i], tokens[i+1])\n",
    "                    pair_counts[pair] = pair_counts.get(pair, 0) + pre_tokens_dict[key]\n",
    "        else:\n",
    "            for key in modified_words:\n",
    "                old_tokens = modified_words[key]\n",
    "                new_tokens = current_tokens[key]\n",
    "                # remove counts for old tokens\n",
    "                for i in range(len(old_tokens)-1):\n",
    "                    pair  = (old_tokens[i], old_tokens[i+1])\n",
    "                    pair_counts[pair] = pair_counts.get(pair, 0) - pre_tokens_dict[key]\n",
    "                    if pair_counts[pair] == 0:\n",
    "                        del pair_counts[pair]\n",
    "                # add counts for new tokens\n",
    "                for i in range(len(new_tokens)-1):\n",
    "                    pair  = (new_tokens[i], new_tokens[i+1])\n",
    "                    pair_counts[pair] = pair_counts.get(pair, 0) + pre_tokens_dict[key]\n",
    "        end_time1 = time.time()        \n",
    "        # find the most frequent pair\n",
    "        # if there are multiple pairs with same frequency, choose one with greatest lex order\n",
    "        def pair_sorter(item):\n",
    "            bytes1 = vocab[item[0][0]]\n",
    "            bytes2 = vocab[item[0][1]]\n",
    "            return (item[1], bytes1, bytes2)\n",
    "        # return first pair\n",
    "        best_pair = max(pair_counts.items(), key=pair_sorter)[0]\n",
    "\n",
    "        # add new token to vocab\n",
    "        vocab[current_size] = vocab[best_pair[0]] + vocab[best_pair[1]]\n",
    "        merges.append((vocab[best_pair[0]], vocab[best_pair[1]]))\n",
    "\n",
    "        start_time2 = time.time()\n",
    "        # update current_tokens\n",
    "        modified_words = {}\n",
    "        for key in pre_tokens_dict.keys():\n",
    "            tokens = current_tokens[key]\n",
    "            i = 0\n",
    "            modified_flag = False\n",
    "            new_tokens = []\n",
    "            n = len(tokens)\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
    "                    new_tokens.append(current_size)\n",
    "                    i += 2\n",
    "                    modified_flag = True\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            if modified_flag:\n",
    "                # store the modified words before updating\n",
    "                modified_words[key] = current_tokens[key]\n",
    "            current_tokens[key] = new_tokens\n",
    "        current_size += 1\n",
    "        end_time2 = time.time()\n",
    "\n",
    "        time_taken1 += end_time1 - start_time1\n",
    "        time_taken2 += end_time2 - start_time2\n",
    "        loop_number += 1\n",
    "    print(f\"Time taken for counting pairs: {time_taken1}, Time taken for updating tokens: {time_taken2}\")\n",
    "    # ajust value order, move special tokens to the front\n",
    "    new_vocab = {}\n",
    "    for i in range(vocab_size):\n",
    "        if i < 256 + len(special_tokens):\n",
    "            if i < len(special_tokens):\n",
    "                new_vocab[i] = vocab[i+256]\n",
    "            else:\n",
    "                new_vocab[i] = vocab[i - len(special_tokens)]\n",
    "        else:\n",
    "            new_vocab[i] = vocab[i]\n",
    "    vocab = new_vocab\n",
    "        \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for counting pairs: 0.04724431037902832, Time taken for updating tokens: 0.7574045658111572\n"
     ]
    }
   ],
   "source": [
    "file_path1 = \"/home/std10/extend/TinyStoriesV2-GPT4-valid.txt\"\n",
    "file_path2 = \"./tests/fixtures/corpus.en\"\n",
    "vocab, merges = train_bpe(file_path2, vocab_size=500, special_tokens=['<|endoftext|>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#寻找merge和 train-bpe-reference.py中merge的不同\n",
    "with open(\"./tests/fixtures/train-bpe-reference-merges.txt\", \"rb\") as f:\n",
    "    reference_merge = []\n",
    "    for line in f:\n",
    "        parts = line.strip().split(b\" \")\n",
    "        reference_merge.append((parts[0], parts[1]))\n",
    "for i in range(len(merges)):\n",
    "    if b' ' in merges[i][0] or b' ' in merges[i][1]:\n",
    "        continue\n",
    "    if merges[i][:2] != reference_merge[i]:\n",
    "        \n",
    "        print(f\"Difference at index {i}:\")\n",
    "        print(f\"  Computed: {merges[i]}\")\n",
    "        print(f\"  Reference: {reference_merge[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, \n",
    "                 vocab: dict[int, bytes], \n",
    "                 merges: list[tuple[bytes, bytes]], \n",
    "                 special_tokens: list[str]=None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens if special_tokens is not None else []\n",
    "        self.token_to_id = {token: idx for idx, token in vocab.items()}\n",
    "    \n",
    "    def from_files(cls, \n",
    "                   vocab_filepath: str, \n",
    "                   merges_filepath: str,\n",
    "                   special_tokens: list[bytes]=None) -> Tokenizer:\n",
    "        vocab: dict[int, bytes] = {}\n",
    "        merges: list[tuple[bytes, bytes]] = []\n",
    "        with open(vocab_filepath, 'rb') as vf:\n",
    "            for line in vf:\n",
    "                token = line.rstrip(b'\\n')\n",
    "                vocab[len(vocab)] = token\n",
    "        with open(merges_filepath, 'rb') as mf:\n",
    "            for line in mf:\n",
    "                token1, token2 = line.rstrip(b'\\n').split(b' ')\n",
    "                merges.append((token1, token2))\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "    \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        parts = re.split(\"|\".join(self.special_tokens), text)\n",
    "        # save all special tokens_ids\n",
    "        \n",
    "        whole_token_ids = []\n",
    "        for part in parts:\n",
    "            PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "            tokens = re.finditer(PAT, part)\n",
    "            token_ids = []\n",
    "            for match in tokens:\n",
    "                token_bytes = match.group(0).encode('utf-8')\n",
    "                token_id = self.token_to_id.get(token_bytes, None)\n",
    "                if token_id is not None:\n",
    "                    token_ids.append(token_id)\n",
    "                else:\n",
    "                    # Apply BPE merges\n",
    "                    sub_tokens = [token_bytes]\n",
    "                    for merge in self.merges:\n",
    "                        new_sub_tokens = []\n",
    "                        i = 0\n",
    "                        while i < len(sub_tokens):\n",
    "                            if i < len(sub_tokens) - 1 and (sub_tokens[i], sub_tokens[i+1]) == merge:\n",
    "                                new_sub_tokens.append(self.vocab[len(self.vocab) + self.merges.index(merge)])\n",
    "                                i += 2\n",
    "                            else:\n",
    "                                new_sub_tokens.append(sub_tokens[i])\n",
    "                                i += 1\n",
    "                        sub_tokens = new_sub_tokens\n",
    "                    for sub_token in sub_tokens:\n",
    "                        sub_token_id = self.token_to_id.get(sub_token, None)\n",
    "                        if sub_token_id is not None:\n",
    "                            token_ids.append(sub_token_id)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Sub-token {sub_token} not found in vocabulary.\")\n",
    "            whole_token_ids.extend(token_ids)\n",
    "        return whole_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_path = '/home/std10/extend/generated_data/tokenized_data_train.npy'\n",
    "tokenized_data = np.load(data_path, allow_pickle=True, mmap_mode='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
